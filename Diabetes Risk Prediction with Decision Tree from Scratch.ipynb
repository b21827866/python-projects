{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63852921",
   "metadata": {},
   "source": [
    "# Diabetes Risk Prediction with Decision Tree from Scratch\n",
    "\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "131dd4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90e67aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('diabetes_data_upload.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a41b492",
   "metadata": {},
   "source": [
    "### Discretization process on continuous attribute\n",
    "\n",
    "Friend of mine which is med student recommend me to split data this way. It gives %100 accuracy without shuffleing and Kfolds\n",
    "\n",
    "Ages splitted to 5 and labeled as 1,2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bdd4d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_age = dataset['Age'].max() #max age in dataset\n",
    "min_age = dataset['Age'].min() #min age in dataset\n",
    "dataset['Age']=pd.cut(x = dataset['Age'],bins = [min_age,30,35,40,55,max_age],labels = [1, 2, 3, 4, 5])\n",
    "#dataset=dataset.sample(frac=1) #shuffling data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a0755",
   "metadata": {},
   "source": [
    "Calculating the entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "891004c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(target_col):\n",
    "\n",
    "    elements,counts = np.unique(target_col,return_counts = True) #finding unique elements\n",
    "\n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))]) #calculating entropy\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac04eb",
   "metadata": {},
   "source": [
    "### Calculation of information gain\n",
    "Finding the imformation gain for every feature. Imformation Gain(Feature) is simply Entropy(Whole dataset) - Entropy(Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4213b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(data,split_attribute_name,target_name=\"class\"):\n",
    "\n",
    "    total_entropy = entropy(data[target_name]) #total entropy of dataset\n",
    "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
    "\n",
    "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
    "\n",
    "    information_gain = total_entropy - Weighted_Entropy #calculating information gain\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a49e25",
   "metadata": {},
   "source": [
    "### ID3 Algo\n",
    "This algorithm will build tree recursively. We have 3 base cases for stopping recursion\n",
    "\n",
    "Base case 1: If all rows in target feature same value\n",
    "\n",
    "Base case 2: When dataset ends\n",
    "\n",
    "Base case 3: When dataset no longer split\n",
    "\n",
    "##### Building the tree\n",
    "Create base node\n",
    "\n",
    "Find whole entropy then calculate info gain for each attribute and pick the largest gain\n",
    "\n",
    "Assign the node of the feature. Grow for each feature value an outgoing branch and add unlabeled nodes at the and\n",
    "\n",
    "Remove the feature with highest info gain\n",
    "\n",
    "Repeat until base case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "127d47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3(data,original_data,features,target_label=\"class\",parent_node_class = None):\n",
    "\n",
    "    if len(np.unique(data[target_label])) <= 1: #base cases\n",
    "        return np.unique(data[target_label])[0]\n",
    "\n",
    "    elif len(data)==0:\n",
    "        return np.unique(original_data[target_label])[np.argmax(np.unique(original_data[target_label],return_counts=True)[1])]\n",
    "\n",
    "    elif len(features) ==0:\n",
    "        return parent_node_class\n",
    "\n",
    "\n",
    "\n",
    "    else: #else build the tree\n",
    "\n",
    "        parent_node_class = np.unique(data[target_label])[np.argmax(np.unique(data[target_label],return_counts=True)[1])]\n",
    "\n",
    "        item_values = [info_gain(data,feature,target_label) for feature in features] #return the information gain values for the features in the dataset\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "\n",
    "        tree = {best_feature:{}}\n",
    "\n",
    "\n",
    "        features = [i for i in features if i != best_feature] #remove best inforamtion gain\n",
    "\n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            subtree = id3(sub_data,dataset,features,target_label,parent_node_class) #recursion\n",
    "\n",
    "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
    "            tree[best_feature][value] = subtree\n",
    "\n",
    "        return(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94278b",
   "metadata": {},
   "source": [
    "Function that returns either prediction is correct or not\n",
    "\n",
    "There is a problem here. Unseen query it may happen that the feature values of these query are not existing in our tree model because non of the training instances has had such a value for this specific feature. If this happens we decided to assign value to Negative. These are our examples of missclassified samples. We can simply drop this samples to improve our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ddf92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query,tree,default = \"Negative\"):\n",
    "\n",
    "    for key in list(query.keys()):\n",
    "        if key in list(tree.keys()):\n",
    "            try:\n",
    "                result = tree[key][query[key]]\n",
    "            except:\n",
    "                return default\n",
    "            result = tree[key][query[key]]\n",
    "            if isinstance(result,dict):\n",
    "                return predict(query,result)\n",
    "\n",
    "            else:\n",
    "                return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f52826",
   "metadata": {},
   "source": [
    "Function that stores predictions. Scikit functions uses this returned values to calculate accuracy, precision etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e656aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tester(data,tree):\n",
    "\n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"])\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,\"Negative\")\n",
    "        #print(predicted.iloc[0:1,i])\n",
    "    return predicted[\"predicted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910664f2",
   "metadata": {},
   "source": [
    "Splitting data into 5 folds with scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "956d4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,:]\n",
    "y = dataset.iloc[:,-1]\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934272aa",
   "metadata": {},
   "source": [
    "There is no general rule for confusion matrix. This creates confusion. So I picked labels myself. Matrix structre showed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "125e0283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[95  9]\n",
      " [ 0  0]]\n",
      "Accuracy: 0.9134615384615384\n",
      "Precision: 1.0\n",
      "Recall: 0.9134615384615384\n",
      "F1 score: 0.9547738693467337\n",
      "\n",
      "Confusion Matrix\n",
      "[[88  8]\n",
      " [ 0  8]]\n",
      "Accuracy: 0.9230769230769231\n",
      "Precision: 1.0\n",
      "Recall: 0.9166666666666666\n",
      "F1 score: 0.9565217391304348\n",
      "\n",
      "Confusion Matrix\n",
      "[[33  0]\n",
      " [ 4 67]]\n",
      "Accuracy: 0.9615384615384616\n",
      "Precision: 0.8918918918918919\n",
      "Recall: 1.0\n",
      "F1 score: 0.9428571428571428\n",
      "\n",
      "Confusion Matrix\n",
      "[[31  0]\n",
      " [10 63]]\n",
      "Accuracy: 0.9038461538461539\n",
      "Precision: 0.7560975609756098\n",
      "Recall: 1.0\n",
      "F1 score: 0.8611111111111112\n",
      "\n",
      "Confusion Matrix\n",
      "[[56  0]\n",
      " [ 0 48]]\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 score: 1.0\n",
      "\n",
      "Average Accuracy 0.9403846153846154\n",
      "Average Precision 0.9295978905735003\n",
      "Average Recall 0.966025641025641\n",
      "Average F1 Score 0.9430527724890844\n"
     ]
    }
   ],
   "source": [
    "avg_accuracy = 0\n",
    "avg_precision = 0\n",
    "avg_recall = 0\n",
    "avg_f1 = 0\n",
    "\n",
    "for train_index , test_index in kf.split(X):\n",
    "    X_train= X.iloc[train_index,:]\n",
    "    y_train = X.iloc[test_index,:]\n",
    "\n",
    "    tree = id3(X_train,X_train,X_train.columns[:-1])\n",
    "    tree_calc = tester(y_train,tree)\n",
    "\n",
    "    confusion_mtx = confusion_matrix(y_train[\"class\"], tree_calc, labels=['Positive','Negative'])\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_mtx)\n",
    "    \"\"\"\n",
    "    matrix structure\n",
    "    TP | FN\n",
    "    --   --\n",
    "    FP | TN\n",
    "    \"\"\"\n",
    "\n",
    "    avg_accuracy += accuracy_score(y_train[\"class\"], tree_calc)\n",
    "    avg_precision += precision_score(y_train[\"class\"], tree_calc, pos_label='Positive')\n",
    "    avg_recall += recall_score(y_train[\"class\"], tree_calc, pos_label='Positive')\n",
    "    avg_f1 += f1_score(y_train[\"class\"], tree_calc, pos_label='Positive')\n",
    "    print(\"Accuracy:\", accuracy_score(y_train[\"class\"], tree_calc))\n",
    "    print(\"Precision:\", precision_score(y_train[\"class\"], tree_calc, pos_label='Positive'))\n",
    "    print(\"Recall:\", recall_score(y_train[\"class\"], tree_calc, pos_label='Positive'))\n",
    "    print(\"F1 score:\", f1_score(y_train[\"class\"], tree_calc, pos_label='Positive'))\n",
    "    print()\n",
    "\n",
    "print(\"Average Accuracy\", avg_accuracy/5)\n",
    "print(\"Average Precision\", avg_precision/5)\n",
    "print(\"Average Recall\", avg_recall/5)\n",
    "print(\"Average F1 Score\", avg_f1/5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa4175c",
   "metadata": {},
   "source": [
    "True Positives (TP) - These are the correctly predicted positive values which means that the value of actual class is positive and the value of predicted class is also positive\n",
    "\n",
    "True Negatives (TN) - These are the correctly predicted negative values which means that the value of actual class is negative and value of predicted class is also negative\n",
    "\n",
    "False Positives (FP) – When actual class is negative and predicted class is positive. E.g. if actual class says this passenger healty but predicted class tells you that this person has a diabet.\n",
    "\n",
    "False Negatives (FN) – When actual class is positive but predicted class in negative. E.g. if actual class value indicates that this person has a diabet and predicted class tells you that person is healty.\n",
    "\n",
    "#### Accuracy\n",
    "Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. Accuracy is a great measure but only when we have symmetric datasets where values of false positive and false negatives are almost same. In our first 4 folds we dont have a symetric FP and FN. So accuracy is not a good model for our data.\n",
    "\n",
    "#### Precision\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as positive, how many actually positive? High precision relates to the low false positive rate. In first two folds we have a 1.0 precision which is nice. The precision score is a useful measure of the success of prediction when the classes are very imbalanced.\n",
    "\n",
    "#### Recall\n",
    "Recall score represents the model’s ability to correctly predict the positives out of actual positives. This is unlike precision which measures how many predictions made by models are actually positive out of all positive predictions made. It measures how good our machine learning model is at identifying all actual positives out of all positives that exist within a dataset. In fold 3 and 4 we have a 1.0 recall score. Recall score is a useful measure of success of prediction when the classes are very imbalanced.\n",
    "\n",
    "#### F1\n",
    "F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, F1 is usually more useful than accuracy when especially we have an uneven class distribution. F1 score is more usefull than accuracy for our data. It balances precision and recall. \n",
    "\n",
    "5th folds predicts %100 true. We can analyze train and set set of this fold and can understand better for improving general prediction percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f3b0c7",
   "metadata": {},
   "source": [
    "# Pruning Decision Tree\n",
    "\n",
    "Pruning is a technique that is used to reduce overfitting. Pruning also simplifies a decision tree by removing the weakest rules. Pruning is often distinguished into:\n",
    "Pre-pruning (early stopping) stops the tree before it has completed classifying the training set,\n",
    "Post-pruning allows the tree to classify the training set perfectly and then prunes the tree.\n",
    "\n",
    "Travel all twigs in the tree\n",
    "\n",
    "Find the twig with the least Information Gain\n",
    "\n",
    "Remove all child nodes of the twig\n",
    "\n",
    "Relabel twig as a Positive or Negative leaf\n",
    "\n",
    "Measure the accuracy value of your decision tree model with removed\n",
    "twig on the validation set (”Current Accuracy”)\n",
    "\n",
    "If ”Current Accuracy ≥ Last Accuracy” : Jump to ”Step1”\n",
    "Else : Revert the last changes done in Step 3,4 and then terminate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e841644c",
   "metadata": {},
   "source": [
    "Pruning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0c1d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_pruning(tree, df_train, df_val):\n",
    "\n",
    "    question = list(tree.keys())[0]\n",
    "    yes_answer, no_answer = tree[question]\n",
    "\n",
    "    # base case\n",
    "    if not isinstance(yes_answer, dict) and not isinstance(no_answer, dict):\n",
    "        return pruning_result(tree, df_train, df_val)\n",
    "\n",
    "    # recursive part\n",
    "    else:\n",
    "        df_train_yes, df_train_no = filter_df(df_train, question)\n",
    "        df_val_yes, df_val_no = filter_df(df_val, question)\n",
    "\n",
    "        if isinstance(yes_answer, dict):\n",
    "            yes_answer = post_pruning(yes_answer, df_train_yes, df_val_yes)\n",
    "\n",
    "        if isinstance(no_answer, dict):\n",
    "            no_answer = post_pruning(no_answer, df_train_no, df_val_no)\n",
    "\n",
    "        tree = {question: [yes_answer, no_answer]}\n",
    "\n",
    "        return pruning_result(tree, df_train, df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e60a2",
   "metadata": {},
   "source": [
    "Helper Functions to pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8db75f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_leaf(df_train):\n",
    "\n",
    "    return info_gain(df_train, df_train.columns[:-1], \"class\")\n",
    "\n",
    "def determine_min_info_gain(df_val, tree):\n",
    "    predictions = tester(df_val, tree)\n",
    "    actual_values = df_val.label\n",
    "\n",
    "    return predictions - actual_values\n",
    "\n",
    "\n",
    "def pruning_result(tree, df_train, df_val):\n",
    "\n",
    "    leaf = determine_leaf(df_train)\n",
    "    errors_leaf = determine_min_info_gain(df_val, leaf)\n",
    "    errors_decision_node = determine_min_info_gain(df_val, tree)\n",
    "\n",
    "    if errors_leaf <= errors_decision_node:\n",
    "        return leaf\n",
    "    else:\n",
    "        return tree\n",
    "\n",
    "def filter_df(df, question):\n",
    "    feature, comparison_operator, value = question.split()\n",
    "\n",
    "    positive = df[df[feature].astype(str) == value]\n",
    "    negative  = df[df[feature].astype(str) != value]\n",
    "\n",
    "    return positive, negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf0570",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2740d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
