{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854dad8e",
   "metadata": {},
   "source": [
    "# SPAM MAIL DETECTION WITH NAIVE BAYES\n",
    "\n",
    "\n",
    "\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf3c4a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb48d08",
   "metadata": {},
   "source": [
    "Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "148bd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('emails.csv') #reading data\n",
    "df.drop_duplicates(inplace=True) #removing duplicate rows\n",
    "\n",
    "spammails = df[df['spam'] == 1]\n",
    "hammails = df[df['spam'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e651e985",
   "metadata": {},
   "source": [
    "Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d26a5e05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text\n",
       "spam      \n",
       "0     4327\n",
       "1     1368"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('spam').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c877a2c6",
   "metadata": {},
   "source": [
    "Function that returns most common three words for Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18104392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_three_words(dataset, ngram, stopword):\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram, max_features=3, stop_words=stopword) #max_features=3 gives us most common 3 words\n",
    "    txt = dataset.text\n",
    "    X = vectorizer.fit_transform(txt)  #transform\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    feature_count = X.toarray().sum(axis=0)\n",
    "    dictionary = dict(zip(feature_names,feature_count)) #creating dictionary\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95061c16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common 3 words in spam mails with stop-words:  {'and': 6517, 'the': 8975, 'to': 8165}\n",
      "Most common 3 words in spam mails without stop-words :  {'business': 844, 'com': 999, 'subject': 1574}\n",
      "Most common 3 words in ham mails with stop-words:  {'and': 20805, 'the': 40935, 'to': 33369}\n",
      "Most common 3 words in ham mails without stop-words :  {'ect': 11410, 'enron': 13329, 'subject': 8545}\n"
     ]
    }
   ],
   "source": [
    "spam_mail_unigram = get_three_words(spammails,(1,1),None)\n",
    "print('Most common 3 words in spam mails with stop-words: ', spam_mail_unigram)\n",
    "spam_mail_unigram_sw = get_three_words(spammails,(1,1), 'english')\n",
    "print('Most common 3 words in spam mails without stop-words : ', spam_mail_unigram_sw)\n",
    "\n",
    "ham_mail_unigram = get_three_words(hammails, (1,1), None)\n",
    "print('Most common 3 words in ham mails with stop-words: ',ham_mail_unigram)\n",
    "ham_mail_unigram_sw = get_three_words(hammails, (1,1), 'english')\n",
    "print('Most common 3 words in ham mails without stop-words : ', ham_mail_unigram_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7499ecc",
   "metadata": {},
   "source": [
    "## PART 1\n",
    "With stop-words, we can see domination of stop-words. We can't decide mail ham or spam from 3 words because they are all the same. From ocurrences of the words persfective, ham mail / spam mail ratio is 3,2. Ocurrence of \"and\" and \"to\" is relatively close when we multiply spam words counts with 3,2. With stop-words, most common 3 words does not gives us a any idea.\n",
    "\n",
    "Without stop-words, spam mail senders focusing on business and direct links(We can get this idea from \"com\"). Novadays big companies uses bridges or click buttons to reach link, but spam mailers puts direct link into the mail. \"ect\" and \"enron\" looks weird but they are the email domains, ECT=enron capital trade, and HOU=Houston.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a8b5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(ngram, stp_wrds):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram, min_df=0.01, stop_words=stp_wrds)\n",
    "    txt = df.text.fillna(\"\")\n",
    "    X = vectorizer.fit_transform(txt) #_transform\n",
    "\n",
    "    arr2 = vectorizer.get_feature_names()\n",
    "    new_df = pd.concat([df, pd.DataFrame(X.A, columns=arr2)], axis=1, join='inner')\n",
    "\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    global train,test\n",
    "    train, test = train_test_split(new_df, test_size=0.2)\n",
    "\n",
    "    global train2,test2\n",
    "    if(ngram == (1,1)):\n",
    "        train.columns.values[0] = \"text1\"\n",
    "        train.columns.values[train.columns.get_loc(\"text\")] = \"text1\"\n",
    "        train.columns.values[0] = \"text\"\n",
    "\n",
    "    train2, test2 = train.copy(), test.copy()\n",
    "    train['text'] = train['text'].str.replace('\\W', ' ', regex=True) #removes punctuation\n",
    "    train['text'] = train['text'].str.lower() #lowercase strings\n",
    "    train['text'] = train['text'].str.split()\n",
    "    global vocabulary\n",
    "    vocabulary = []\n",
    "    for text in train['text']:\n",
    "       for word in text:\n",
    "          vocabulary.append(word)\n",
    "\n",
    "    vocabulary = list(set(vocabulary))\n",
    "    global spam_mails, ham_mails\n",
    "    spam_mails = train[train['spam'] == 1]\n",
    "    ham_mails = train[train['spam'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b40f1",
   "metadata": {},
   "source": [
    "#### Bayes Theorem\n",
    "\n",
    "If we know the probability of occuring A when B has already occured i.e. **P(A|B)** but we want to know the reverse, probability of occuring B when A has already occured i.e. p(B|A), in that case we will use Bayes theroem which states: **P(A|B) = P(A,B) / P(B) = P(B|A) P(A) / p(B)**\n",
    "\n",
    "Event B can be further split into two mutually exclusive events, \"B and A\" and \"B and not A\" **P(B) = P(B,A) + P(B, not A)**\n",
    "\n",
    "so we can write-\n",
    "\n",
    "**P(A|B) = P(B|A) P(A) / {P(B|A) P(A) + P(B | not A)P(not A)}** and this is how bayes theroes is usually written\n",
    "\n",
    "Lets now use bayes theroem to create the naive Bayes clasifier We will be using the example of spam filter to classify a mail as spam or non spam.\n",
    "\n",
    "Lets say S is the event that a mail is spam and D is the event that the mail contains the word \"Discount\". Now using Bayes theorem, we can find the probabilty of a mail being spam if it contains the word discount\n",
    "\n",
    "**P(S|D) = P(D|S) P(S) / {P(D|S) P(S) + P(D | not S) P(not S)}**\n",
    "\n",
    "Here numerator is probability that mail is spam and contains Discount and denominator is just the probability that the mail contains Discount. Now if a mail being spam and not spam are equally likely events then **P(S) = P (not S) =0.5**\n",
    "\n",
    "Therefore **P(S|D) = P(D|S) / {P(D|S) + P(D | not S)}**\n",
    "\n",
    "so if 60% of the spam mails have the word Discount and only 2% of the non spam mails have the word discount in it then **P(S|D)** = *(0.60/0.62)= 96.77%*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25dd864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes():\n",
    "    global p_spam, p_ham, parameters_spam, parameters_ham\n",
    "    p_spam = len(spam_mails) / len(train)\n",
    "    p_ham = len(ham_mails) / len(train)\n",
    "    # n_Spam\n",
    "    n_words_per_spam_message = spam_mails['text'].apply(len)\n",
    "    n_spam = n_words_per_spam_message.sum()\n",
    "    # n_Ham\n",
    "    n_words_per_ham_message = ham_mails['text'].apply(len)\n",
    "    n_ham = n_words_per_ham_message.sum()\n",
    "    # n_Vocabulary\n",
    "    n_vocabulary = len(vocabulary)\n",
    "    alpha = 1 #laplace smoothing\n",
    "    #Initiate parameters\n",
    "    parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "    parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "    # Calculate parameters\n",
    "    for word in vocabulary:\n",
    "        if(word in spam_mails and word != \"text\"):\n",
    "            n_word_given_spam = spam_mails[word].sum() # spam_messages already defined\n",
    "            p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "            parameters_spam[word] = p_word_given_spam\n",
    "\n",
    "            n_word_given_ham = ham_mails[word].sum() # ham_messages already defined\n",
    "            p_word_given_ham =(n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "            parameters_ham[word] = p_word_given_ham\n",
    "            #print(p_word_given_ham)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86fb80a",
   "metadata": {},
   "source": [
    "Function that classifies mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90ed490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower().split()\n",
    "\n",
    "    p_spam_given_message = math.log2(p_spam)\n",
    "    p_ham_given_message = math.log2(p_ham)\n",
    "\n",
    "    for word in message:\n",
    "        if (word in parameters_spam):\n",
    "            if(parameters_spam[word] != 0):\n",
    "                p_spam_given_message += math.log2(parameters_spam[word])\n",
    "        if (word in parameters_ham):\n",
    "            if(parameters_ham[word] !=0):\n",
    "                p_ham_given_message += math.log2(parameters_ham[word])\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 0\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 #equal probility classified as spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8dbc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score():\n",
    "    test['prediction'] = test['text'].apply(classify_test_set)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy_score(test['spam'], test['prediction']))\n",
    "    print(\"Precision:\", precision_score(test['spam'], test['prediction']))\n",
    "    print(\"Recall:\", recall_score(test['spam'], test['prediction']))\n",
    "    print(\"F1 score:\", f1_score(test['spam'], test['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "885fad17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram scores\n",
      "Accuracy: 0.9744042365401588\n",
      "Precision: 0.9878542510121457\n",
      "Recall: 0.9037037037037037\n",
      "F1 score: 0.9439071566731141\n",
      "\n",
      "Unigram scores without stopwords\n",
      "Accuracy: 0.9779346866725508\n",
      "Precision: 0.9871794871794872\n",
      "Recall: 0.9130434782608695\n",
      "F1 score: 0.948665297741273\n",
      "\n",
      "Bigram scores\n",
      "Accuracy: 0.7625772285966461\n",
      "Precision: 1.0\n",
      "Recall: 0.014652014652014652\n",
      "F1 score: 0.02888086642599278\n",
      "\n",
      "Bigram scores without stopwords\n",
      "Accuracy: 0.7811120917917035\n",
      "Precision: 1.0\n",
      "Recall: 0.027450980392156862\n",
      "F1 score: 0.0534351145038168\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigram scores\")\n",
    "bow((1,1), None)\n",
    "naive_bayes()\n",
    "print_score()\n",
    "print()\n",
    "print(\"Unigram scores without stopwords\")\n",
    "bow((1,1), \"english\")\n",
    "naive_bayes()\n",
    "print_score()\n",
    "print()\n",
    "print(\"Bigram scores\")\n",
    "bow((2,2), None)\n",
    "naive_bayes()\n",
    "print_score()\n",
    "print()\n",
    "print(\"Bigram scores without stopwords\")\n",
    "bow((2,2), \"english\")\n",
    "naive_bayes()\n",
    "print_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a1335",
   "metadata": {},
   "source": [
    "#### Unigram Analysis\n",
    "For our data removing stopwords don't have any effect on our scores. Removing stopwords will reduce total word size in mails and it will help us to calculate predictions faster. We have relatively close accuracy, precision, recall and f1 scores. False positives and false negatives are balanced. Selecting accuracy score will be better for our data\n",
    "\n",
    "#### Bigram Analysis\n",
    "Using bigram changes our scores dramaticly. We can say for wrong predictions algorithm classifies *spam* mails as a *ham*. This is why we have a amazingly low recall score. But our algorithm doesn't predicts *ham* mails as a *spam*. This gives a 1.0 precision score. The reason is ham mails has a better structe. Gramaticly correct sentences has a more common adjacent words. From spam mail persfective, senteces are messy and has a lot of typos. Bigram is a good model when predicting ham mails but not good for predicting spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8db6d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_mails2 = train2[train2['spam'] == 1]\n",
    "ham_mails2 = train2[train2['spam'] == 0]\n",
    "\n",
    "def td_idf(dataframe, type, stp_wrds):\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stp_wrds)\n",
    "    tfidf_vector = tfidf_vectorizer.fit_transform(dataframe[\"text\"])\n",
    "    tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=dataframe[\"text\"], columns=tfidf_vectorizer.get_feature_names())\n",
    "    tfidf_df.loc[\"0\"] = (tfidf_df > 0).sum()\n",
    "    tfidf_df.drop(\"text\", axis=1)\n",
    "\n",
    "    tfidf_df = tfidf_df.sort_index()\n",
    "    if(type == \"presence\"):\n",
    "        print()\n",
    "        tfidf_df = tfidf_df.sort_values(by = \"0\", axis = 1, ascending=False )\n",
    "    if(type == \"absence\"):\n",
    "        tfidf_df = tfidf_df.sort_values(by = \"0\", axis = 1)\n",
    "    print(tfidf_df.loc[\"0\"].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eec4c2c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 words whose presence most strongly predicts that the mail is spam\n",
      "\n",
      "subject    1113.0\n",
      "to          946.0\n",
      "the         885.0\n",
      "and         802.0\n",
      "you         791.0\n",
      "of          788.0\n",
      "your        779.0\n",
      "for         738.0\n",
      "is          716.0\n",
      "in          691.0\n",
      "Name: 0, dtype: float64\n",
      "10 words whose absence most strongly predicts that the mail is spam\n",
      "interact         1.0\n",
      "stow             1.0\n",
      "hsa              1.0\n",
      "hsdl             1.0\n",
      "storyclick       1.0\n",
      "storyacquired    1.0\n",
      "storms           1.0\n",
      "https            1.0\n",
      "hue              1.0\n",
      "huffman          1.0\n",
      "Name: 0, dtype: float64\n",
      "10 words whose presence most strongly predicts that the mail is ham\n",
      "\n",
      "subject    3418.0\n",
      "to         3180.0\n",
      "the        3142.0\n",
      "and        2802.0\n",
      "for        2733.0\n",
      "you        2730.0\n",
      "of         2608.0\n",
      "on         2560.0\n",
      "in         2529.0\n",
      "is         2394.0\n",
      "Name: 0, dtype: float64\n",
      "10 words whose absence most strongly predicts that the mail is ham\n",
      "invention        1.0\n",
      "knights          1.0\n",
      "knobs            1.0\n",
      "knocked          1.0\n",
      "knocks           1.0\n",
      "knossos          1.0\n",
      "knots            1.0\n",
      "knott            1.0\n",
      "knowledgeable    1.0\n",
      "koffice          1.0\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"10 words whose presence most strongly predicts that the mail is spam\")\n",
    "td_idf(spam_mails2, \"presence\", None)\n",
    "print(\"10 words whose absence most strongly predicts that the mail is spam\")\n",
    "td_idf(spam_mails2, \"absence\", None)\n",
    "print(\"10 words whose presence most strongly predicts that the mail is ham\")\n",
    "td_idf(ham_mails2, \"presence\", None)\n",
    "print(\"10 words whose absence most strongly predicts that the mail is ham\")\n",
    "td_idf(ham_mails2, \"absence\", None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b53813",
   "metadata": {},
   "source": [
    "Presence of words are dominated by stop-words. These will not helpfull for our algorithm. We can rather choose using absence for our algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7601dc2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 words whose presence most strongly predicts that the mail is ham without stopwprds\n",
      "\n",
      "subject     3418.0\n",
      "vince       2195.0\n",
      "enron       2044.0\n",
      "cc          1724.0\n",
      "kaminski    1554.0\n",
      "thanks      1416.0\n",
      "2000        1383.0\n",
      "pm          1340.0\n",
      "ect         1278.0\n",
      "know        1224.0\n",
      "Name: 0, dtype: float64\n",
      "10 words whose presence most strongly predicts that the mail is spam without stopwprds\n",
      "\n",
      "subject        1113.0\n",
      "com             368.0\n",
      "http            313.0\n",
      "just            299.0\n",
      "business        269.0\n",
      "click           262.0\n",
      "information     259.0\n",
      "time            252.0\n",
      "email           249.0\n",
      "best            246.0\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"10 words whose presence most strongly predicts that the mail is ham without stopwprds\")\n",
    "td_idf(ham_mails2, \"presence\", \"english\")\n",
    "print(\"10 words whose presence most strongly predicts that the mail is spam without stopwprds\")\n",
    "td_idf(spam_mails2, \"presence\", \"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28a80f0",
   "metadata": {},
   "source": [
    "Without stopwords now we have a usefull presence of words. We can use these words to predict faster. When we see these words\n",
    "we can decide is mail spam or ham earlier. We can gain time.\n",
    "\n",
    "**Why might it make sense to remove stop words when interpreting the model?**\n",
    "If we have a huge amount of data or limited memory, removing stopwords will help a lot. Also we remove the low-level information from our text in order to give more focus to the important information. For our dataset removing stopwords don't have any negative effect. Helped us to calculate unigram faster and increased bigram score tiny bit.\n",
    "\n",
    "\n",
    "**Why might it make sense to keep stop words?**\n",
    "We should remove stopwords only if they don’t add any new information to our problem.For example, if we are training a model that can perform the sentiment analysis task, we might not remove the stop words.\n",
    "\n",
    "\n",
    "Movie review: “The movie was not good at all.”\n",
    "\n",
    "\n",
    "Text after removal of stop words: “movie good”\n",
    "\n",
    "We can clearly see that the review for the movie was negative. However, after the removal of stop words, the review became positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe2b173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
